{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "\n",
    "def login(driver, base_url, username, password):\n",
    "    \"\"\"Log into the website and authenticate the session.\"\"\"\n",
    "    driver.get(base_url)\n",
    "\n",
    "    # Wait for redirection and Shibboleth login page to load\n",
    "    sleep(3)  # Adjust the sleep time as needed\n",
    "\n",
    "    # Inspect the page and find the correct IDs or names for these elements\n",
    "    username_field_id = 'username'  # Replace with the actual ID or name\n",
    "    password_field_id = 'password'  # Replace with the actual ID or name\n",
    "\n",
    "    # Fill in the username and password\n",
    "    driver.find_element(By.ID, username_field_id).send_keys(username)\n",
    "    driver.find_element(By.ID, password_field_id).send_keys(password)\n",
    "\n",
    "    # Submit the form\n",
    "    driver.find_element(By.ID, password_field_id).send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for the authentication to complete and the target page to load\n",
    "    sleep(10)  # Adjust the sleep time as needed\n",
    "\n",
    "def crawl_article(driver, url, filename):\n",
    "    \"\"\"Crawl an article, extract all HTML, and save it to a specified file.\"\"\"\n",
    "    driver.get(url)\n",
    "    sleep(3)  # Adjust the sleep time as needed\n",
    "\n",
    "    html = driver.page_source\n",
    "\n",
    "    # Save the HTML content to the specified file\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html)\n",
    "\n",
    "def create_directory(path):\n",
    "    \"\"\"Create a directory if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access your environment variable\n",
    "username = os.getenv('MY_APP_USERNAME')\n",
    "password = os.getenv('MY_APP_PASSWORD')\n",
    "\n",
    "# Main script\n",
    "base_url = 'https://www-microbiologyresearch-org.ezp3.lib.umn.edu'\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "login(driver, base_url, username, password)\n",
    "\n",
    "del username, password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data\n",
    "with open('articles.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "base_dir = 'articles'  # Base directory name\n",
    "create_directory(base_dir)  # Create the base directory\n",
    "\n",
    "# Iterate through the JSON data\n",
    "for year, volumes in data.items():\n",
    "    for volume, pages in volumes.items():\n",
    "        for page, articles in pages.items():\n",
    "            page_dir = os.path.join(base_dir, year, volume, page)\n",
    "            create_directory(page_dir)  # Create the directory for each page\n",
    "\n",
    "            for article_number, article in enumerate(articles, start=1):\n",
    "                # Skip the article if 'filename' key exists\n",
    "                if 'filename' in article:\n",
    "                    continue\n",
    "\n",
    "                formatted_number = f\"{article_number:03}\"  # Zero-padded number\n",
    "                filename = f\"{year}_{volume}_{page}_{formatted_number}.html\"\n",
    "                full_path = os.path.join(page_dir, filename)\n",
    "                url = article['link']\n",
    "\n",
    "                # Crawl the article and save the HTML\n",
    "                crawl_article(driver, url, full_path)\n",
    "\n",
    "                # Update JSON data with the filename\n",
    "                article['filename'] = filename\n",
    "\n",
    "            # Save the updated JSON data after each page is processed\n",
    "            with open('articles.json', 'w') as file:\n",
    "                json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
